{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    pipeline,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Clemson Univsersity is cool, but Tillman was a racist Thurman hid his black children. I like that it is close to the Blue Mounttains, which is good for Hiking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "label_list = [\n",
    "     \"O\",       # Outside of a named entity\n",
    "     \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
    "     \"I-MISC\",  # Miscellaneous entity\n",
    "     \"B-PER\",   # Beginning of a person's name right after another person's name\n",
    "     \"I-PER\",   # Person's name\n",
    "     \"B-ORG\",   # Beginning of an organisation right after another organisation\n",
    "     \"I-ORG\",   # Organisation\n",
    "     \"B-LOC\",   # Beginning of a location right after another location\n",
    "     \"I-LOC\"    # Location\n",
    "]\n",
    "# Bit of a hack to get the tokens with the special tokens\n",
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "outputs = model(inputs)[0]\n",
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [[token, label_list[prediction]] for token, prediction in zip(tokens, predictions[0].numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in enumerate(pred):\n",
    "    print(i,j)\n",
    "#     if j[1] is \"O\":\n",
    "#         pred.remove(j)\n",
    "# #         continue\n",
    "#     else:\n",
    "#         j[0] = j[0].replace('#','')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
